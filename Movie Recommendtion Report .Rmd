##### Title: "Movie Ratings Project" || Author: "Amit Ranjan" #####

### Introduction
### In this project, we will be using leaning from the HarvardX Professional Certificate in Data Science program on edX to build a movie recommendation system.
### We will first create the data sets with code that has been provided by the course from HarvardX. Thenafter, we will explore the data and its distribution to get a better understanding of the data. We will then train a machine learning algorithm on the **edx set** to finally help us make recommendations on the **validation set**.

### Our output will be 4 different files:
### 1. A *submission.csv* file that will contain the list of movies and our recommendations
### 2. A Report in RMD file format
### 3. A simple R script that generates our recommendations
### 4. A report in PDF format, which we will obtain by publishing this R Code in HTML and printing as PDF
 

# Create edX set, Validation Set, and Submission File
### load all libraries to ensure everything will run smoothly.

Now, we run the code provided by the the course, which creates an **edx** data frame and a **validation** data frame.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Note: this process could take a couple of minutes to download and install all libraries (total download around 100+ MB)
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(ggrepel)) install.packages("ggrepel", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")

library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(gridExtra)
library(dslabs)
library(data.table)
library(ggrepel)
library(ggthemes)
library(tidyverse)
library(caret)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
# Validation set will be 10% of MovieLens data
set.seed(100)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
# Learners will develop their algorithms on the edx set
# For grading, learners will run algorithm on validation set to generate ratings
validation <- validation %>% select(-rating)
# Ratings will go into the CSV submission file below:
write.csv(validation %>% select(userId, movieId) %>% mutate(rating = NA),
          "submission.csv", na = "", row.names=FALSE)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


# Exploratory Data Analysis

This section repeart the data explorartion with the answers to the quizz online, and then we will verify some of the data quality which we learnt during class followed by additional data analysis.

### Quiz from edX

In our environment, we now have two objects. The **edx set** has `r nrow(edx)` observations by `r length(edx)` variables. The **validation set** has `r nrow(validation)` observations by `r length(validation)` variables. 

Unsurprisingly, the 5 variables included in the **validation set** are also present in the **edx set**, which also has a variable called **rating**, which is the purpose of this paper: predicting the rating for the validation set.

The distribution of ratings given in the **edx set** is as follows.

```{r message=FALSE, warning=FALSE, paged.print=FALSE, results="asis"}
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
library(knitr)
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
library(kableExtra)
ratings_freqency <- edx %>% 
  group_by(rating) %>% 
  count() %>% 
  rename("Rating"=rating, "Frequency"=n) %>% 
  mutate("Of total"=paste(round(100*Frequency/nrow(edx),2),"%",sep=""))
kable(ratings_freqency, align = rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

The **edx set** is unique on every row, but the number of movies represented is not equal to the number of rows. To count the number of unique movies represented in the set, we use this quick piece of code. Note that we are using the **movieId** variable in case there might be some typos in the **title** variable.

```{r}
length(unique(edx$movieId))
```

We can make the same analysis for the number of unique users.

```{r}
length(unique(edx$userId))
```

From a genre perspective, we are asked to find out how many ratings there are for movies that fit the following genres: drama, comedy, thriller, and romance. Of course, a movie might fit different genre.

Create a new data frame that will stipulate, for each row, which genre that movie fits. We will then count by genre.

```{r}
genre_analysis <- edx %>% mutate(Is.Drama = str_detect(genres, "Drama"),
                                 Is.Comedy = str_detect(genres,"Comedy"),
                                 Is.Thriller = str_detect(genres, "Thriller"),
                                 Is.Romance = str_detect(genres, "Romance"))
genre_analysis <- data_frame("Genre"=c("Drama", "Comedy", "Thriller", "Romance"),
                             "Ratings included"=c(sum(genre_analysis$Is.Drama),
                                                  sum(genre_analysis$Is.Comedy),
                                                  sum(genre_analysis$Is.Thriller), 
                                                  sum(genre_analysis$Is.Romance)))
kable(genre_analysis, align = rep("c",2)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Now, if we want to see which movie has the greatest number of ratings, we use this piece of code.

```{r}
movies_by_number_of_rankings <- edx %>% 
  group_by(title) %>% 
  count() %>% 
  ungroup() %>% 
  arrange(-n) %>% 
  rename("Movie"=title, "Ratings"=n)
kable(head(movies_by_number_of_rankings), align = rep("c",2)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

To see which ratings are given the most often, we can reorder a table we made above.

```{r}
ratings_freqency <- ratings_freqency %>% arrange(-Frequency)
kable(ratings_freqency, align = rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

We can also evaluate how frequent are the "half star ratings".

```{r}
half_vs_full_ratings <- edx %>% 
  mutate("Type"=ifelse(rating %in% c(1,2,3,4,5),"Full","Half")) %>% 
  group_by(Type) %>% 
  select(Type) %>% 
  count() %>% 
  rename("Ratings"=n) %>% 
  mutate("Of total"=paste(round(100*Ratings/nrow(edx),2),"%",sep=""))
kable(half_vs_full_ratings, align = rep("c",2)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```


##########################################################################################################################
# Exploratory analysis
##########################################################################################################################

Validate following observation in dataset 
1) Any Null
2) How many rating


```{r message=FALSE, warning=FALSE, paged.print=FALSE}
head(edx)

# Check for any missing values
anyNA(edx)

# Quick summary of the dataset
summary(edx)

edx %>% summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId))


# 10 different rating scores, lowest is 0.5 and highest is 5
unique(edx$rating)

```


validate following data quality learning
1) Some moved are not rated
2) Some users are always rating lower than other
3) Movies made during different time gets dfferent rating 
4) each user has provided different ratings for different movies

```{r, echo=FALSE}
keep <- edx %>% 
  count(movieId) %>% 
  top_n(5, n) %>% 
  .$movieId
tab <- edx %>% 
  filter(movieId%in%keep) %>% 
  filter(userId %in% c(13:25)) %>% 
  select(userId, title, rating) %>% 
  mutate(title = str_remove(title, ", The"),
         title = str_remove(title, ":.*")) %>%
  spread(title, rating)
kable(tab) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

We can get a sense of the sparcity of the data by looking at a matrix for a random sample of 100 movies and 100 users with yellow indicating a user/movie combination for which we have a rating. As we can see here, the data is very sparse.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
if(!require(rafalib)) install.packages("rafalib", repos = "http://cran.us.r-project.org")
library(rafalib)
users <- sample(unique(edx$userId), 100)
rafalib::mypar()
edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users") %>% 
  abline(h=0:100+0.5, v=0:100+0.5, col = "lightgrey")
```

Of course, some movies receive a lot more ratings than others. This is not surprising as blockbusters are expected to be rated more frequently than niche movies. We can get a sense of the distribution.

```{r}
edx %>% 
  count(movieId) %>%
  arrange(-n) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  labs(title="Movies by rating count", y="Movies", x="Number of ratings")
```

Of course, the same can be said about users: some are much more active than others.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
edx %>% 
  count(userId) %>%
  arrange(-n) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  scale_x_log10()+
  labs(title="Users by rating count", y="Users", x="Number of ratings")
```

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# Review Training rating distribution
edx %>% 
  ggplot(aes(rating)) + 
  geom_histogram(binwidth=0.2, color="darkblue", fill="lightblue") + 
  ggtitle("Rating Distribution (Training")
```

Let's verify  whether different users tend to rate movies more conservative than others

```{r}
Avg_by_user <- edx %>% 
  group_by(userId) %>% 
  summarize(avg=mean(rating)) 
```

On average, users give an average score of `r mean(Avg_by_user$avg)`. That being said, there is indeed a distribution around this mean.

```{r}  
Avg_by_user %>% ggplot(aes(avg)) +
  geom_histogram(bins = 30, color = "black")+
  labs(title="Distribution of average score by users", x="Average score", y="Number of users")
```

Extract release year from title into a separate field
Number of movies per year/decade


```{r message=FALSE, warning=FALSE, paged.print=FALSE}
edx <- edx %>% mutate(releaseyear = as.numeric(str_extract(str_extract(title, "[/(]\\d{4}[/)]$"), regex("\\d{4}"))),title = str_remove(title, "[/(]\\d{4}[/)]$"))

movies_per_year <- edx %>%
  select(movieId, releaseyear) %>% 
  group_by(releaseyear) %>% 
  summarise(count = n())  %>%
  arrange(releaseyear)

# Create a plot for new  dataset to verify the changes
movies_per_year %>%
  ggplot(aes(x = releaseyear, y = count)) +
  geom_line(color="blue")
```

What were the most popular movie genres year by year?

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

genresByYear <- edx %>% 
  separate_rows(genres, sep = "\\|") %>% 
  select(movieId, releaseyear, genres) %>% 
  group_by(releaseyear, genres) %>% 
  summarise(count = n()) %>% arrange(desc(releaseyear))
```

Different periods show certain genres being more popular during those periods
It will be very hard to incorporate genre into overall prediction given this fact

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

ggplot(genresByYear, aes(x = releaseyear, y = count)) + 
  geom_col(aes(fill = genres), position = 'dodge') + 
  theme_hc() + 
  ylab('Number of Movies') + 
  ggtitle('Popularity per year by Genre')

# View the number of times each user has reviewed movies  
edx %>% count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 15, binwidth=0.1, color="black", show.legend = FALSE, aes(fill = cut(n, 100))) + 
  scale_x_log10() + 
  ggtitle("User Reviews")
# Most users have reviewed less than 200 movies
```
View release year vs rating

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

edx %>% group_by(releaseyear) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(releaseyear, rating)) +
  geom_point() +
  theme_hc() + 
  geom_smooth() +
  ggtitle("Release Year vs. Rating")
# Older "classics" get higher ratings. This could allow us to penalize a movie based on release year
# by a calculated weight.
```

Remove any variables we no longer need to reduce memory footprint

```{r message=FALSE, warning=FALSE, paged.print=FALSE}

rm(movies_per_year, genresByYear)
```
# Our goal is to is predict better than 50/50 (coin toss)
# Given we have 10 possible ratings, random chance would at best
# give us 1/10 odds, or 10% accuracy



# Training Machine Learning Algorithm

### Creating the Training and Test Set

For this portion of the paper, we will randomly select 20% of the **edx set** to serve as our test set.

```{r}
set.seed(100)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.05, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
```

To make sure we don't include users and movies in the test set that do not appear in the training set, we remove these entries using the semi_join function:
  
```{r}
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
```


### Mesure of Success

Typically, we would evaluate our algorithm based on the Loss function, or residual mean squared error (RMSE). We define $y_{u,i}$ as the rating for movie $i$ by user $u$ and denote our prediction with $\hat{y}_{i,u}$. The RMSE is then defined as: 
  
$$
  \mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }
$$
  with $N$ being the number of user/movie combinations and the sum occurring over all these combinations.

Let's write a function that computes the **RMSE** for vectors of ratings and their corresponding predictions:

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

Basically, for RMSE greater than 1, it means that on average we are off by more than one star. We therefore try to minimize RMSE.

We are also told that we will be evaluated based on accuracy, not RMSE. This means that we are tasked with making categorical predictions as opposed to numerical predictions. The only values we should be predicting are therefore **0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, or 5**.

The final measure of our success will be calculated as follows:

$$
\mbox{Accuracy}=\frac{Correct\ Predictions}{Total\ Predictions}
$$
Let's write a function that computes the **accuracy** for vectors of ratings and their corresponding predictions:
  
```{r}
accuracy <- function(true_ratings, predicted_ratings){
  mean(true_ratings==predicted_ratings)
}
```

Let us test the functions:
  
```{r}
true_ratings <- c(1,2,4.5,4,5,0.5)
predicted_ratings <- c(1,2.5,4.5,4,2,1)
RMSE(true_ratings,predicted_ratings)
accuracy(true_ratings,predicted_ratings)
rm(true_ratings,predicted_ratings)
```

### Our Approach

Given our task, we need to train a machine learning algorithm that will make categorical predictions, not simple continuous prediction. Therefore, we will take a two steps approach.

First, we will create a model that minimizes our RMSE. By drawing on the edX course material, it is likely that the following model will be one that takes into account a "regularized" movie effect and the user-specific effect. Namely, we will create the following model.

$$
  Y_{u,i} = \mu + \hat{b}_i(\lambda_i) + \hat{b}_u(\lambda_u) + \varepsilon_{u,i}
$$
  In this model, we assume that a given rating (which is categorical in nature) is the sum of $\mu$ the average rating, $\hat{b}_i(\lambda)$ the regularized movie effect (see below for details), $\hat{b}_u(\lambda)$ the regularized user-specific effect and $\varepsilon_{u,i}$ the residual.

Our first task will be to estimate the first three terms and minimize the RMSE. This has the effect of getting us as close to the answer as possible. At this point, however our accuracy will still be close to 0% because we won't have round numbers. 

Then, we will take this list of predictions, and try to estimate the $\varepsilon_{u,i}$ using different machine learning algorithms.

This residual term will help us integrate other effects that influence the rating. It will also bring our predictions to the categorical variable we are targetting.

### The Linear Model

Let us simply check what our success metrics would be if we were to predict the average for each observation.

```{r}
average_rating <- mean(train_set$rating)
naiveRMSE <- RMSE(test_set$rating, average_rating)
naiveAccuracy <- accuracy(test_set$rating, average_rating)
```

As we go along, we will be comparing different approaches. Let's start by creating a results table with this naive approach:
  
```{r}
rmse_results <- data_frame(Method = "Just the average", RMSE = naiveRMSE, Accuracy =naiveAccuracy)
kable(rmse_results,align=rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Of course, we obtain an accuracy of 0 since the average is not a round number. If we try that we get:
  
```{r}
rounded_average_rating <- round(average_rating/0.5)*0.5
roundednaiveRMSE <- RMSE(test_set$rating,rounded_average_rating)
roundednaiveAccuracy <- accuracy(test_set$rating,rounded_average_rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "Just the average (rounded)",
                                     RMSE=roundednaiveRMSE,
                                     Accuracy = roundednaiveAccuracy))
kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```
We can see here that we have increased our accuracy to about `r paste(round(100*rmse_results[2,3],2),"%",sep="")`, but our RMSE has deteriorated, which is not surprising.

At this time, we are trying to minimize the residual term, so we will focus on the RMSE, and worry about the accuracy later.

#### Movie-effect

We can now factor in the fact that certain movies tend to get much better ratings than others. We call this the **movie_effect**.

For each movie, the movie effect is calculated as the average of $Y_{u,i} - \hat{\mu}$ for each movie $i$.

We calculate it using this piece of code:
  
```{r}
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(movie_effect = mean(rating - average_rating))
```

We can see that these estimates vary substantially:
  
```{r}
movie_avgs %>% qplot(movie_effect, geom ="histogram", bins = 10, data = ., color = I("black"))
```

Since our average rating $\mu$ is about 3.5, a 1.5 movie_effect $b_i$ means a perfect score of 5. 

We can now try to estimate each observation of the test set using this movie_effect. To be clear, this is the model we are trying, where $\mu$ is the average rating, $b_i$ is the movie_effect and $\varepsilon_{u,i}$ is the error term:
  
$$
  Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
$$
  
  
```{r}
predicted_ratings <- average_rating + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$movie_effect
movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating)
movie_effect_Accuracy <- accuracy(test_set$rating,predicted_ratings)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Movie Effect",
                                     RMSE=movie_effect_RMSE,
                                     Accuracy = movie_effect_Accuracy))
kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

As we can see, compared to the rounded average, we have improved our RMSE. However, we have only improved by about `r round(rmse_results[1,2]-rmse_results[3,2],3)`.

#### Regularizing the Movie Effect

At this point, we make the hypothesis that some of the large movie effects in our data set are due to a very low number of ratings being available for certain movies. For instance, one movie might have been rated only once and given 5 stars. It would therefore have a very high $b_i$ of ~1.5. There is a case to be made that this shouldn't be the case as we have limited information regarding that movie.

First, let's create a database that connects `movieId` to movie title:
  
```{r}
movie_titles <- edx %>% 
  select(movieId, title) %>%
  distinct()
```

Here are the 10 best movies according to our estimate and how often they were rated:
  
```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(movie_effect)) %>% 
  select(title, movie_effect,n) %>%
  rename("Title"=title,"Movie Effect"=movie_effect,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

And here are the 10 worst:
  
```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(movie_effect) %>% 
  select(title, movie_effect,n) %>%
  rename("Title"=title,"Movie Effect"=movie_effect,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Our hypothesis was right: we hare heavily benefiting or penalizing obscure movies when we actually have very little data points for them. These are noisy estimates that we should not trust, especially when it comes to prediction. Large errors can increase our RMSE, so we would rather be conservative when unsure.

We will now estimate the **regularized_movie_effect** such that:
  
$$
  Regularized\ Movie\ Effect = \hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)
$$
  
where $n_i$ is the number of ratings made for movie $i$.

The intuition here is that as $n_i$ gets bigger, the impact of adding $\lambda$ diminishes. However, for small values of $n_i$, the presence of $\lambda$ reduces the estimate of the movie effect $\hat{b}_i(\lambda)$ .

Let's compute these regularized estimates of **regularized_movie_effect** using 
$\lambda=3$. Later, we will obtimize this term.

```{r}
lambda <- 3
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(reg_movie_effects = sum(rating - average_rating)/(n()+lambda), n_i = n()) 
```

To see how the estimates shrink, let's make a plot of the regularized estimates versus the least squares estimates.

```{r regularization-shrinkage}
data_frame("Original Movie Effect" = movie_avgs$movie_effect, 
           "Regularized Movie Effect" = movie_reg_avgs$reg_movie_effects, 
           n = movie_reg_avgs$n_i) %>%
  ggplot(aes(`Original Movie Effect`, `Regularized Movie Effect`, size=sqrt(n))) + 
  geom_point(shape=1, alpha=0.5)
```

Let's look at the top 10 best movies based on $\hat{b}_i(\lambda)$:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_reg_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(desc(reg_movie_effects)) %>% 
  select(title, reg_movie_effects,n) %>%
  rename("Title"=title,"Reg. Movie Effect"=reg_movie_effects,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

This makes a lot more sense. These movies are widely known as some of the best ever made.

And now the 10 worst:

```{r}
train_set %>% count(movieId) %>% 
  left_join(movie_reg_avgs, by ="movieId") %>%
  left_join(movie_titles, by="movieId") %>%
  arrange(reg_movie_effects) %>% 
  select(title, reg_movie_effects,n) %>%
  rename("Title"=title,"Reg. Movie Effect"=reg_movie_effects,"Times Rated"=n) %>% 
  slice(1:10) %>%  
  kable() %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Let's see if this improves our results further:
  
```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = average_rating + reg_movie_effects) %>%
  .$pred
reg_movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 
reg_movie_effect_Accuracy <- accuracy(predicted_ratings, test_set$rating) 
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Regularized Movie Effect",
                                     RMSE=reg_movie_effect_RMSE,
                                     Accuracy = reg_movie_effect_Accuracy))
kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

We can see that the improvement from regularization, although quite sensical, is not very large.

This might be due to the fact that we arbitrarily selected $\lambda=3$.

###### Optimizing Lambda for the Movie Effect

Because $\lambda$ is a tuning parameter, we can use cross-validation to choose it. We will do so on the training set only.

```{r}
lambdas <- seq(0, 10, 0.25)
mu <- mean(train_set$rating)
just_the_sum <- train_set %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())
rmses <- sapply(lambdas, function(l){
  predicted_ratings <- test_set %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    .$pred
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)  
```

Therefore, for the best results, we should use this lambda.

```{r}
lambda_movie <- lambdas[which.min(rmses)]
lambda_movie
```

Let's now use this optimized lambda to see how it improves our results.

```{r}
lambda <- lambda_movie
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(reg_movie_effects = sum(rating - average_rating)/(n()+lambda), n_i = n()) 
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  mutate(pred = average_rating + reg_movie_effects) %>%
  .$pred
reg_movie_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 
reg_movie_effect_Accuracy <- accuracy(predicted_ratings, test_set$rating) 
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Optimized Regularized Movie Effect",
                                     RMSE=reg_movie_effect_RMSE,
                                     Accuracy = reg_movie_effect_Accuracy))
kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

There is a slight improvement.

#### Adding the User-Specific Effect

Of course, we know that some users also rate movies differently based on their personal characteristics.

```{r}
train_set %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")+
  labs(title="Count of users by average rating")
```

Here we can calculate the **user_effect** which is a user-specific effect once the **regularized_movie_effect** has been taken into consideration.

To be clear, this is the model we are trying, where $\mu$ is the average rating, $\hat{b}_i(\lambda)$ is the regularized_movie_effect, $b_u$ is the user-specific effect and $\varepsilon_{u,i}$ is the error term:
  
$$
Y_{u,i} = \mu + \hat{b}_i(\lambda) +b_u + \varepsilon_{u,i}
$$

```{r}
user_avgs <- train_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(user_effect = mean(rating - average_rating - reg_movie_effects))
```

Let's create predictions see how it improves our model.

```{r}
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = average_rating + reg_movie_effects + user_effect) %>%
  .$pred
movieanduser_effect_RMSE <- RMSE(predicted_ratings, test_set$rating) 
movieanduser_effect_Accuracy <- accuracy(predicted_ratings, test_set$rating) 
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Optimized Regularized Movie and User Effect",
                                     RMSE=movieanduser_effect_RMSE,
                                     Accuracy = movieanduser_effect_Accuracy))
kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

#### Regularizing the User Effect

For the same reasons we regularized the movie effect, we want to also regularized the user effect. In the code below, we find the best $\lambda_u$ to use for the final model:
  
$$
  Y_{u,i} = \mu + \hat{b}_i(\lambda_i) + \hat{b}_u(\lambda_u) + \varepsilon_{u,i}
$$
```{r best-lambdas}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  
  reg_b_i <- movie_reg_avgs %>% 
    group_by(movieId) %>%
    summarize(reg_b_i = reg_movie_effects)
  
  reg_b_u <- train_set %>% 
    left_join(reg_b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(reg_b_u = sum(rating - reg_b_i - mu)/(n()+l))
  predicted_ratings <- 
    test_set %>% 
    left_join(reg_b_i, by = "movieId") %>%
    left_join(reg_b_u, by = "userId") %>%
    mutate(pred = mu + reg_b_i + reg_b_u) %>%
    .$pred
  
  return(RMSE(predicted_ratings, test_set$rating))
})
qplot(lambdas, rmses)
lambda_user <- lambdas[which.min(rmses)]
```

Therefore, the best $\lambda_u$ to use would be `r lambda_user` and the RMSE we expect would be `r min(rmses)`; very good indeed!
  
  Let's build our final predictions for the linear model.

```{r}
user_reg_avg <- train_set %>% 
    left_join(movie_reg_avgs, by="movieId") %>%
    group_by(userId) %>%
    summarize(reg_user_effects = sum(rating - reg_movie_effects - average_rating)/(n()+lambda_user))
predicted_ratings <- test_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  left_join(user_reg_avg, by='userId') %>%
  mutate(pred = average_rating + reg_movie_effects + reg_user_effects) %>%
  .$pred
final_RMSE <- RMSE(predicted_ratings, test_set$rating) 
final_Accuracy <- accuracy(predicted_ratings, test_set$rating) 
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method= "With Optimized Regularized Movie and Optimized Regularized User Effect",
                                     RMSE=final_RMSE,
                                     Accuracy = final_Accuracy))
kable(rmse_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

#### The Residuals

Of course, as we can see in the summary table, our accuracy is still 0% because we do not have round numbers. Let's take a look at the distribution of residuals.

```{r}
residuals <- data_frame(Residuals = predicted_ratings-test_set$rating, True_Rating = test_set$rating, Predictions=predicted_ratings )
residuals %>% ggplot(aes(Residuals))+
  geom_histogram(bins = 30, color = "black") + 
  labs(title="Distribution of Residuals", y="Count", x="Residual")
```

We can also look at the relationship between the true ratings and our predictions.

```{r}
fun_mean <- function(x){
  return(data.frame(y=mean(x),2,label=mean(x,na.rm=T)))}
residuals %>% ggplot(aes(x=True_Rating,y=Predictions, group=True_Rating))+
  geom_boxplot()+
  labs(title="Distribution of prediction by True Rating", y="Predictions", x="True Ratings")
```

Here, we see that in the low ratings, we tend to predict too high a rating and in the high ratings we tend to predict too low a rating.

### Using Machine Learning to Estimate $\varepsilon_{u,i}$ in Our Model

Let us remind ourselves where we are. We have the following model, in which $\mu$ is the average rating, $\hat{b}_i(\lambda_i)$ is the optimized regularized movie effect using a $\lambda_i$ of `r lambda_movie`, $\hat{b}_u(\lambda_u)$ is the optimized regularized user effect using a $\lambda_u$ of `r lambda_user`, $\varepsilon_{u,i}$ is the residual term we just analyzed and $Y_{u,i}$ is the true rating.

$$
  Y_{u,i} = \mu + \hat{b}_i(\lambda_i) + \hat{b}_u(\lambda_u) + \varepsilon_{u,i}
$$
  We currently have a RMSE of `r round(min(rmses),5)`.

We now want to do two things:
  
  1. Ensure that all of our predictions are categorical since we will be evaluted based on accuracy

2. Tease out any other effect that might be burried into the $\varepsilon_{u,i}$
  
  As we go, we will be recording our results in the following table:
  
```{r}
accuracy_results <- data_frame(Method = "Optimized Linear Model", 
                               RMSE = RMSE(residuals$True_Rating,residuals$Predictions),
                               Accuracy = accuracy(residuals$True_Rating,residuals$Predictions))
kable(accuracy_results,align=rep("c",3)) %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

If we were to simply round each prediction to the nearest 0.5, we would get:
  
```{r}
accuracy_results <- bind_rows(accuracy_results,
                              data_frame(Method= "Naive Rounding",
                                         RMSE=RMSE(residuals$True_Rating,round(residuals$Predictions/0.5)*0.5),
                                         Accuracy = accuracy(residuals$True_Rating,round(residuals$Predictions/0.5)*0.5)))
kable(accuracy_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

Our accuracy has improved to about `r paste(round(accuracy_results[2,3]*100,2),"%",sep="")`.

Surely, we can do better with a "smarter" approach. 

#### Using a Decision Tree to Create Rounded Predictions

Before moving forward, we free up some memory.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
rm(Avg_by_user,genre_analysis,Genres_Stats,half_vs_full_ratings,just_the_sum,means,movie_avgs,movie_titles,movies_by_number_of_rankings,ratings_freqency,residuals,tab,test_index,user_avgs, current_genre,final_Accuracy,final_RMSE,i,keep,lambda,lambdas,movie_effect_Accuracy,movie_effect_RMSE,movieanduser_effect_Accuracy,movieanduser_effect_RMSE,mu,naiveAccuracy,naiveRMSE,reg_movie_effect_Accuracy,reg_movie_effect_RMSE,rmses,rounded_average_rating,roundednaiveAccuracy,roundednaiveRMSE,users)
```


First, let us create a list of prediction for the training set.

```{r}
predicted_ratings_training <- train_set %>% 
  left_join(movie_reg_avgs, by='movieId') %>%
  left_join(user_reg_avg, by='userId') %>%
  mutate(pred = average_rating + reg_movie_effects + reg_user_effects) %>%
  .$pred
train_set <- train_set %>% 
  mutate(Predictions = predicted_ratings_training)
test_set <- test_set %>% 
  mutate(Predictions = predicted_ratings)
```

In the training set, our RMSE is:
  
```{r}
RMSE(train_set$rating,train_set$Predictions)
```

If we were to simply round we would get the following accuracy:
  
```{r}
accuracy(train_set$rating, round(train_set$Predictions/0.5)*0.5)
```

Quite similar to what we had in the test set.

Before training machine learning algorithms, we need to take a sample of the training set, otherwise our computer runs out of RAM.

```{r}
set.seed(100)
sub_index <- createDataPartition(y = train_set$rating, times = 1, p = 0.05, list = FALSE)
sub_train_set <- train_set[sub_index,]
```


Let's now train a decision tree, while transforming the ratings into categorical data so that the decision tree gives us categorical data as well.

```{r}
train_rpart <- train(as.character(rating) ~ Predictions, 
                     method = "rpart",
                     data = sub_train_set)
```

The decision tree looks as follows:

```{r}
plot(train_rpart$finalModel,margin = 0.1)
text(train_rpart$finalModel, cex = 0.75)
```

And our results on the test set would be:

```{r}
tree_predicted_values <- predict(train_rpart,test_set)
tree_RMSE <- RMSE(as.numeric(as.character(tree_predicted_values)),test_set$rating)
tree_accuracy <- accuracy(tree_predicted_values,test_set$rating)
accuracy_results <- bind_rows(accuracy_results,
                          data_frame(Method= "Using Decision Tree with Predicted Values",
                                     RMSE=tree_RMSE,
                                     Accuracy = tree_accuracy))
kable(accuracy_results,align=rep("c",3), caption = "Metrics calculated on test set only") %>% 
  kable_styling(full_width = F) %>% 
  column_spec(1,bold=T,border_right = T)
```

We can see here that we have increased our accuracy quite a bit, to `r paste(round(100*accuracy_results[3,3],2),"%",sep="")`.

Let us now re-create our sub-training-set.

```{r}
set.seed(755)
sub_index <- createDataPartition(y = train_set$rating, times = 1, p = 0.01, list = FALSE)
sub_train_set <- train_set[sub_index,]
sub_train_set <- sub_train_set %>% 
  select(rating,Predictions,timestamp)
sub_train_set$rating <- as.character(sub_train_set$rating)
```

#### Trying Various Algorithms to Maximize Accuracy

Here, we will train a vast number of algorithms on the sub_training_set. We will then create a matrix of prediction on the full training set. 

```{r}
models <- c( "lda", "naive_bayes","knn", "kknn", "Rborist")
```

```{r}
fits <- lapply(models, function(model){ 
	print(model)
	train(rating ~ ., 
	      method = model, 
	      data = sub_train_set)
}) 
    
names(fits) <- models
```

## Other Techniques
Over the last 2 weeks I have tried and tested numerous models using different algorithms, including Naive Bayes, SVD, Truncated SVD, Matrix Factorization, Recommenderlab and more. Random Forest on a small subset of 500,000 rows consistently scored above 80% accuracy,
but due to resource constraints and limitations built into various R Studio and Libraries this could not be extended to the full training set of 9 million datasets.

## Conclusion
As stated earlier, I tested Naive Bayes, Random Forest, Tensorflow Neural Networks, PCA, SVD, Recommenderlab, KNN, Kmeans, and various other models and  algorithms. Some were fast but the accuracy poor. Others were accurate on smaller sets (Random Forest) but simply could not scale to this data set size, and offered no reliable means to split and combine.

While still requiring a lot of RAM, hence I was not able to include more % of data into training or validation neither I could add more predictors. I re-ran few models using training subsets of 5% and 10% splits.

The 10 split required 20+ GB of RAM while to 5 split managed to fit into 16GB space. I also encountered issues with rStudio running out for vector space, even though there was memory available on system. A typical machine used for machine learning is often equipped with more resources, especially RAM and multiple GPU's, so our requirements are not above the norm.

In conclusion, utilizing only 3 elements of the data set (user, movie, rating) we were able to predict. Finding the right algorithm that 
suits the existing data set is often more useful than exploring augmentation of the data to reach your own conclusions, such as including or introducing an artificial bias or weights, as we did with the naive models.

The biggest challenge turned out to be the dataset size, not the prediction problem, and we found a suitable algorithm for our data that could be adapted to break the training set into manageable smaller subsets to be recombined into a single predictor model without noticeable loss in accuracy.


